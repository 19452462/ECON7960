{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "163a1a9c9d4fc3f61c766fdf860af902bd8e762c"
   },
   "source": [
    "# Workshop 6: Google Analytics Revenue Determination Crucial Factors\n",
    "\n",
    "\n",
    "\n",
    "### Contents of this Kernel\n",
    "\n",
    "1. Problem Statement  \n",
    "2. Dataset Preparation  \n",
    "3. Simple Look into Visitor Profiles\n",
    "4. Boosting: catboost, lightgbmboost and xgboost \n",
    " \n",
    "\n",
    "## 1. Problem Statement \n",
    "\n",
    "In this exercise https://www.kaggle.com/c/ga-customer-revenue-prediction , the aim is to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. The exercise here is an explosition of the data and look into the GA data about the personas of the visitors and the analysis might lead to a better use of marketing budgets for those companies who choose to use data analysis on top of GA data. \n",
    "\n",
    "This exercise is to introduce a tool help to extract the important factors determinate your business objectives given you have done the exercise in WOrkshop 4 enable to format the data for analysis.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "#### Q1: Seek out insights of the personas of the visitors to form crucial insights\n",
    "#### Q2: Compare different kinds of boosting algorithms\n",
    "\n",
    "As the first step, lets load the required libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to download the text.csv (6G bit) from below link to the same directory for working in this Jupyter notebook exercises.\n",
    "\n",
    "Since github does not allow to upload file more than 2Gbit, the data file is placed on my Google drive as follows: https://drive.google.com/file/d/1euXsx5hfq0N5mowMyo3ecqEGcwzHalpT/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "source": [
    "If your PC does not have plotly, you need to find way to install plotly to call in init_notebook_mode, iplot, plotly.graph_objs and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "source": [
    "## 2. Understanding the Dataset\n",
    "\n",
    "The data is shared in csv format. The csv files contains some filed with json objects. The description about dataset fields is given  https://www.kaggle.com/c/ga-customer-revenue-prediction/data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "source": [
    "### 2.1 Dataset Preparation\n",
    "\n",
    "Lets read the dataset in csv format and unwrap the json fields. Students can reference on https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html \n",
    "and\n",
    "https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "df = pd.read_csv(\"test.csv\", nrows=10000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "outputs": [],
   "source": [
    "# See the variables \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "outputs": [],
   "source": [
    "# See the variables \n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d53b5bdd24383e8eb78b163fa3047cd53febfc7d"
   },
   "source": [
    "#### it is noted in below how servers record some steamless data over interact with clients in the JSON format\n",
    "\n",
    "\n",
    "Only \"device\", \"geoNetwork\", \"totals\", \"trafficSource\" are JSON objects whereas \"customDimensions\" and \"hits\" are different data nested dictionary objects. One will notice that the data strcuture of series of \"customDimensions\" and \"hits\" are more complicated, in this exercise, they are not included in the analysis.\n",
    "\n",
    "\"hits\" record all the pages footprints. The Google Analytics data variables description one can find some details about https://support.google.com/analytics/answer/3437719?hl=en. Google develops a BigQuery SQL API that one can access its GA data for further AB design testing, reference https://towardsdatascience.com/how-to-query-and-calculate-google-analytics-data-in-bigquery-cab8fc4f396 (which we will come back to look into it in lecture 8-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pandas.read_csv(converters) \n",
    "Dict of functions for converting values in certain columns. Keys can either be integers or column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path='test.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        # \"normalize\" convert semi-structured JSON data into a flat table.\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        # using \".\" rather than \"_\" for subcolumn\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {csv_path}. Shape: {df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the whole data may take hours depending the computing efficiency of your machine, it is suggested to limit to data size of 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_df(\"test.csv\", 300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "673e7c7367a4a5980560baa125e43376d64cd5ff"
   },
   "source": [
    "### 2.2 Dataset Snapshot\n",
    "\n",
    "Lets view the snapshot of the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9e761c2ef85fd6c9c94d7a155d639d6a5424d209"
   },
   "outputs": [],
   "source": [
    "print (\"There are \" + str(train.shape[0]) + \" rows and \" + str(train.shape[1]) + \" raw columns in this dataset\")\n",
    "\n",
    "print (\"Snapshot: \", train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4d55abc37cefb9a341b3271b1c6f703c347237e"
   },
   "source": [
    "### 2.3 Missing Values Percentage\n",
    "\n",
    "From the snapshot we can observe that there are many missing values in the dataset. Let's plot the missing values percentage for columns having missing values. \n",
    "\n",
    "> The following graph shows only those columns having missing values, all other columns are fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "44dfb5f91100da2e89986be56c0ccab162e089e4"
   },
   "outputs": [],
   "source": [
    "miss_per = {}\n",
    "for k, v in dict(train.isna().sum(axis=0)).items():\n",
    "    if v == 0:\n",
    "        continue\n",
    "    miss_per[k] = 100 * float(v) / len(train)\n",
    "    \n",
    "import operator \n",
    "sorted_x = sorted(miss_per.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print (\"There are \" + str(len(miss_per)) + \" columns with missing values\")\n",
    "\n",
    "kys = [_[0] for _ in sorted_x][::-1]\n",
    "vls = [_[1] for _ in sorted_x][::-1]\n",
    "trace1 = go.Bar(y = kys, orientation=\"h\" , x = vls, marker=dict(color=\"#d6a5ff\"))\n",
    "layout = go.Layout(title=\"Missing Values Percentage\", \n",
    "                   xaxis=dict(title=\"Missing Percentage\"), \n",
    "                   height=400, margin=dict(l=300, r=300))\n",
    "figure = go.Figure(data = [trace1], layout = layout)\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "129c51c109ffa9659c881549cf0b29568d3d2993"
   },
   "source": [
    "> - So we can observe that there are some columns in the dataset having very large number of missing values. \n",
    "\n",
    "\n",
    "## 3. Visitor Profile \n",
    "\n",
    "Lets create the visitor profile by aggregating the rows for every customer. \n",
    "\n",
    "### 3.1 Visitor Profile Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "46841c2028ea593e5d77538e7d9b6648cd1b5472"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "agg_dict = {}\n",
    "for col in [\"totals.bounces\", \"totals.hits\", \"totals.newVisits\", \"totals.pageviews\", \"totals.transactionRevenue\"]:\n",
    "    train[col] = train[col].astype('float')\n",
    "    agg_dict[col] = \"sum\"\n",
    "tmp = train.groupby(\"fullVisitorId\").agg(agg_dict).reset_index()\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62a62c5484e808b19c79907c9f1590e95effc1f7"
   },
   "source": [
    "### 3.2 Total Transactions Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c60b224592c69e15e4e286d83cec1de595f317ab"
   },
   "outputs": [],
   "source": [
    "non_zero = tmp[tmp[\"totals.transactionRevenue\"] > 0][\"totals.transactionRevenue\"]\n",
    "print (\"There are \" + str(len(non_zero)) + \" visitors in the train dataset having non zero total transaction revenue\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(non_zero)\n",
    "plt.title(\"Distribution of Non Zero Total Transactions\");\n",
    "plt.xlabel(\"Total Transactions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e54cfc3f5751bb7e26bf46bac570e471809789c9"
   },
   "source": [
    "Lets take the natural log on the transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e0e4b4c6a17803195d4c9f1ebedfddfb670a20a4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(np.log1p(non_zero))\n",
    "plt.title(\"Log Distribution of Non Zero Total Transactions\");\n",
    "plt.xlabel(\"Log - Total Transactions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "30f66ab7332348bf1a6837dd272caf5090be0d8f"
   },
   "source": [
    "### 3.3 Visitor Profile Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "15d9c375b9672344d4b320ed6cd918b3644c3cb0"
   },
   "outputs": [],
   "source": [
    "def getbin_hits(x):\n",
    "    if x < 5:\n",
    "        return \"1-5\"\n",
    "    elif x < 10:\n",
    "        return \"5-10\"\n",
    "    elif x < 30:\n",
    "        return \"10-30\"\n",
    "    elif x < 50:\n",
    "        return \"30-50\"\n",
    "    elif x < 100:\n",
    "        return \"50-100\"\n",
    "    else:\n",
    "        return \"100+\"\n",
    "\n",
    "tmp[\"total.hits_bin\"] = tmp[\"totals.hits\"].apply(getbin_hits)\n",
    "tmp[\"totals.bounces_bin\"] = tmp[\"totals.bounces\"].apply(lambda x : str(x) if x <= 5 else \"5+\")\n",
    "tmp[\"totals.pageviews_bin\"] = tmp[\"totals.pageviews\"].apply(lambda x : str(x) if x <= 50 else \"50+\")\n",
    "\n",
    "t1 = tmp[\"total.hits_bin\"].value_counts()\n",
    "t2 = tmp[\"totals.bounces_bin\"].value_counts()\n",
    "t3 = tmp[\"totals.newVisits\"].value_counts()\n",
    "t4 = tmp[\"totals.pageviews_bin\"].value_counts()\n",
    "\n",
    "fig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Total Hits per User\", \"Total Bounces per User\", \n",
    "                                                         \"Total NewVistits per User\", \"Total PageViews per User\"], print_grid=False)\n",
    "\n",
    "tr1 = go.Bar(x = t1.index[:20], y = t1.values[:20])\n",
    "tr2 = go.Bar(x = t2.index[:20], y = t2.values[:20])\n",
    "tr3 = go.Bar(x = t3.index[:20], y = t3.values[:20])\n",
    "tr4 = go.Bar(x = t4.index, y = t4.values)\n",
    "\n",
    "fig.append_trace(tr1, 1, 1)\n",
    "fig.append_trace(tr2, 1, 2)\n",
    "fig.append_trace(tr3, 2, 1)\n",
    "fig.append_trace(tr4, 2, 2)\n",
    "\n",
    "fig['layout'].update(height=700, showlegend=False)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n",
    "\n",
    "def load_preprocessed_dfs(drop_full_visitor_id=True):\n",
    "    \"\"\"\n",
    "    Loads files `TRAIN`, `TEST` and `Y` generated by preprocess() into variables\n",
    "    \"\"\"\n",
    "    X_train = pd.read_csv(TRAIN, converters={'fullVisitorId': str})\n",
    "    X_test = pd.read_csv(TEST, converters={'fullVisitorId': str})\n",
    "    y_train = pd.read_csv(Y, names=['LogRevenue']).T.squeeze()\n",
    "    \n",
    "    # This is the only `object` column, we drop it for train and evaluation\n",
    "    if drop_full_visitor_id: \n",
    "        X_train = X_train.drop(['fullVisitorId'], axis=1)\n",
    "        X_test = X_test.drop(['fullVisitorId'], axis=1)\n",
    "    return X_train, y_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test = load_preprocessed_dfs()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Validation shape: {X_val.shape}\")\n",
    "print(f\"Test (submit) shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(X_train, y_train, X_val, y_val, X_test):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 40,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"bagging_fraction\" : 0.6,\n",
    "        \"feature_fraction\" : 0.6,\n",
    "        \"bagging_frequency\" : 6,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    \n",
    "    lgb_train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    lgb_val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(params, lgb_train_data, \n",
    "                      num_boost_round=5000,\n",
    "                      valid_sets=[lgb_train_data, lgb_val_data],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=500)\n",
    "\n",
    "    y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "    y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    y_pred_submit = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    print(f\"LGBM: RMSE val: {rmse(y_val, y_pred_val)}  - RMSE train: {rmse(y_train, y_pred_train)}\")\n",
    "    return y_pred_submit, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train LGBM and generate predictions\n",
    "lgb_preds, lgb_model = run_lgb(X_train, y_train, X_val, y_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LightGBM features importance...\")\n",
    "gain = lgb_model.feature_importance('gain')\n",
    "featureimp = pd.DataFrame({'feature': lgb_model.feature_name(), \n",
    "                   'split': lgb_model.feature_importance('split'), \n",
    "                   'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "print(featureimp[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(X_train, y_train, X_val, y_val, X_test):\n",
    "    params = {'objective': 'reg:linear',\n",
    "              'eval_metric': 'rmse',\n",
    "              'eta': 0.001,\n",
    "              'max_depth': 10,\n",
    "              'subsample': 0.6,\n",
    "              'colsample_bytree': 0.6,\n",
    "              'alpha':0.001,\n",
    "              'random_state': 42,\n",
    "              'silent': True}\n",
    "\n",
    "    xgb_train_data = xgb.DMatrix(X_train, y_train)\n",
    "    xgb_val_data = xgb.DMatrix(X_val, y_val)\n",
    "    xgb_submit_data = xgb.DMatrix(X_test)\n",
    "\n",
    "    model = xgb.train(params, xgb_train_data, \n",
    "                      num_boost_round=2000, \n",
    "                      evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],\n",
    "                      early_stopping_rounds=100, \n",
    "                      verbose_eval=500\n",
    "                     )\n",
    "\n",
    "    y_pred_train = model.predict(xgb_train_data, ntree_limit=model.best_ntree_limit)\n",
    "    y_pred_val = model.predict(xgb_val_data, ntree_limit=model.best_ntree_limit)\n",
    "    y_pred_submit = model.predict(xgb_submit_data, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    print(f\"XGB : RMSE val: {rmse(y_val, y_pred_val)}  - RMSE train: {rmse(y_train, y_pred_train)}\")\n",
    "    return y_pred_submit, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_preds, xgb_model = run_xgb(X_train, y_train, X_val, y_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_catboost(X_train, y_train, X_val, y_val, X_test):\n",
    "    model = CatBoostRegressor(iterations=1000,\n",
    "                             learning_rate=0.05,\n",
    "                             depth=10,\n",
    "                             eval_metric='RMSE',\n",
    "                             random_seed = 42,\n",
    "                             bagging_temperature = 0.2,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 50,\n",
    "                             od_wait=20)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=(X_val, y_val),\n",
    "              use_best_model=True,\n",
    "              verbose=True)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_submit = model.predict(X_test)\n",
    "\n",
    "    print(f\"CatB: RMSE val: {rmse(y_val, y_pred_val)}  - RMSE train: {rmse(y_train, y_pred_train)}\")\n",
    "    return y_pred_submit, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train Catboost and generate predictions\n",
    "cat_preds, cat_model = run_catboost(X_train, y_train, X_val, y_val,  X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is currently being reconstructed!\n",
    "ensemble_preds_70_30_00 = 0.7 * lgb_preds + 0.3 * cat_preds + 0.0 * xgb_preds \n",
    "ensemble_preds_70_25_05 = 0.7 * lgb_preds + 0.25 * cat_preds + 0.05 * xgb_preds \n",
    "def submit(predictions, filename='submit.csv'):\n",
    "    \"\"\"\n",
    "    Takes a (804684,) 1d-array of predictions and generates a submission file named filename\n",
    "    \"\"\"\n",
    "    _, _, X_submit = load_preprocessed_dfs(drop_full_visitor_id=False)\n",
    "    submission = X_submit[['fullVisitorId']].copy()\n",
    "    \n",
    "    submission.loc[:, 'PredictedLogRevenue'] = predictions\n",
    "    grouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\n",
    "    grouped_test.to_csv(filename,index=False)\n",
    "\n",
    "submit(lgb_preds, \"submit-lgb.csv\")\n",
    "# Note: I disabled XGB to make the notebook run faster\n",
    "submit(xgb_preds, \"submit-xgb.csv\")\n",
    "submit(cat_preds, \"submit-cat.csv\")\n",
    "submit(ensemble_preds_70_30_00, \"submit-ensemble-70_30_00.csv\")\n",
    "submit(ensemble_preds_70_25_05, \"submit-ensemble-70_25_05.csv\")\n",
    "\n",
    "ensemble_preds_70_30_00_pos = np.where(ensemble_preds_70_30_00 < 0, 0, ensemble_preds_70_30_00)\n",
    "submit(ensemble_preds_70_30_00_pos, \"submit-ensemble-70_30_00-positive.csv\")\n",
    "\n",
    "ensemble_preds_70_25_05_pos = np.where(ensemble_preds_70_25_05 < 0, 0, ensemble_preds_70_25_05)\n",
    "submit(ensemble_preds_70_25_05_pos, \"submit-ensemble-70_25_05-positive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
