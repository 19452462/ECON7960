{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB0 Simulation to Demonstrate the Type II Error and the Power Analysis of the A/B Test \n",
    "\n",
    "In a A/B test hypothesis testing situation in order to understand the power analysis, it is important to understand three related concepts: significance level, Type I/II errors and the effect size.\n",
    "\n",
    "In hypothesis testing, significance level (often denoted as Greek letter alpha) is the probability of rejecting the null hypothesis (H0), which it is usually conventional wisdom. A metric closely related to the significance level is the p-value, which is the probability of obtaining a result at least as extreme (a result against the null hypothesis), even if the H0 was true. What does that mean in practice? In case of drawing a random sample from a population, \n",
    "\n",
    "It is always possible that the observed extreme/ unlikely outcome would have occurred only due to sampling error.\n",
    "The result of an experiment (or for example a linear regression coefficient) is statistically significant when the associated p-value is smaller than the chosen probability. The significance level, meaning the probability allowed in the experiment that wrongly rejects the conventional wisdom which is true, should be specified before setting up the study and depends on the field of research/business needs. \n",
    "\n",
    "The second concept worth mentioning is the types of errors we can commit while statistically testing a hypothesis. When we reject a true H0 we are talking about a Type I error (false positive). This is also another error connected to the significance level (mentioned above). The other case occurs when we fail to reject a false H0, which is considered to be a Type II error (false negative). \n",
    "\n",
    "The last concept to consider it the effect size, related to the population which is the quantified magnitude of Type I and Type II errors present in the sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programme for the Power Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Type II Error \n",
    "\n",
    "##### Assume you draw sample size of 100 from a normal distributions with one mean of 0.2 with variances of 1. \n",
    "\n",
    "Based on the sample, one run tests of 5% signifinance against level the null hypothesis of the mean is zero. Since the mean is 0.1, the hull hypthesis mean is zero, it should be rejected. But to sampling, it is possible that it cannot be rejected.\n",
    "\n",
    "Code below is to simulate 10000 sample to estimate the probability of the test \"not\" to be rejected (the false negative), the type II error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from numpy import array\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "from statsmodels.stats.power import TTestIndPower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical power\n",
    "\n",
    "The key concepts related to power analysis, statistical power, of a hypothesis test is simply the probability that the given test correctly rejects the null hypothesis (which means the somehow as accepting the H1), when the alternative is in fact true.\n",
    "\n",
    "Higher statistical power of an experiment means lower probability of committing a Type II error. It also means higher probability of detecting an effect when there is an effect to detect (true positive). This can be illustrated by the following formula:\n",
    "\n",
    "Power = Pr(reject H0 | H1 is true) = 1 - Pr(fail to reject H0 | H0 is false) = 1 - Type II Error\n",
    "\n",
    "In practice, results from experiments with too little power will lead to wrong conclusions, which in turn will affect the decision-making process. That is why only results with an acceptable level of power should be taken into consideration. It is quite common to design experiments with power level of 80%, which translates to a 20% probability of committing a Type II error.\n",
    "\n",
    "The idea of power analysis can be brought down to the following: by having defining three out of four metrics, we estimate the missing one. This comes in handy in two ways:\n",
    "\n",
    "In designing an experiment, we can assume what \"level of significance\" (alpha), \"power\" (1- beta) and \"effect size\" is acceptable to us and — as a result — estimate \"how big a sample\" we need to gather for such an experiment to yield valid results or when validate an experiment, one can see if, given the used sample size, effect size and significance level, what is the probability of the level of a Type II error is acceptable from the business "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here illustrate the Type II error and the effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type II Error and the Effect Size  \n",
    "\n",
    "n_per_group = 100\n",
    "# effect size = 0.2\n",
    "group_means = [0.0, 0.2]\n",
    "group_sigmas = [1.0, 1.0]\n",
    "\n",
    "# there are only 2 groups\n",
    "n_groups = len(group_means)\n",
    "\n",
    "# number of simulations\n",
    "n_sims = 100000\n",
    "\n",
    "# definite a file to store the p value for each simulation\n",
    "sim_p = np.empty(n_sims)\n",
    "sim_p.fill(np.nan)\n",
    "t_value = np.empty(n_sims)\n",
    "t_value.fill(np.nan)\n",
    "\n",
    "for i_sim in range(n_sims):\n",
    "\n",
    "    data = np.empty([n_per_group, n_groups])\n",
    "    data.fill(np.nan)\n",
    "\n",
    "    # simulate the data for this 'experiment'\n",
    "    for i_group in range(n_groups):\n",
    "\n",
    "        data[:, i_group] = np.random.normal(\n",
    "            loc=group_means[i_group],\n",
    "            scale=group_sigmas[i_group],\n",
    "            size=n_per_group)\n",
    "\n",
    "    result = ttest_ind(data[:, 1], data[:, 0], alternative='larger', usevar='pooled', value=0)\n",
    "# alternative = ‘two-sided’ (default): H1: difference in means not equal to value  \n",
    "    t_value[i_sim] = result[0]\n",
    "    sim_p[i_sim] = result[1]\n",
    "   \n",
    "print(t_value[0:10])\n",
    "print(sim_p[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type II Error and the Size of the Effect \n",
    "# number of simulations where the null was rejected at 5% significance level\n",
    "n_rej = np.sum(sim_p < 0.05)\n",
    "\n",
    "prop_rej = 1 - (n_rej / float(n_sims))\n",
    "\n",
    "print(\"TYPE II ERROR: \", prop_rej)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here illustrate the power of the test and the effect size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power vs. Size Effect \n",
    "\n",
    "n_per_group = 100\n",
    "# effect size = 0.2\n",
    "group_means = [0, 0.2]\n",
    "group_sigmas = [1.0, 1.0]\n",
    "\n",
    "# there are only 2 groups\n",
    "n_groups = len(group_means)\n",
    "\n",
    "# number of simulations\n",
    "n_sims = 100000\n",
    "\n",
    "# definite a file to store the p value for each simulation\n",
    "sim_p = np.empty(n_sims)\n",
    "sim_p.fill(np.nan)\n",
    "t_value = np.empty(n_sims)\n",
    "t_value.fill(np.nan)\n",
    "for i_sim in range(n_sims):\n",
    "\n",
    "    data = np.empty([n_per_group, n_groups])\n",
    "    data.fill(np.nan)\n",
    "\n",
    "    # simulate the data for this 'experiment'\n",
    "    for i_group in range(n_groups):\n",
    "\n",
    "        data[:, i_group] = np.random.normal(\n",
    "            loc=group_means[i_group],\n",
    "            scale=group_sigmas[i_group],\n",
    "            size=n_per_group)\n",
    "\n",
    "    result = ttest_ind(data[:, 1], data[:, 0], alternative='larger', usevar='pooled', value=0)\n",
    "# alternative = ‘two-sided’ (default): H1: difference in means not equal to value    \n",
    "    t_value[i_sim] = result[0]\n",
    "    sim_p[i_sim] = result[1]\n",
    "   \n",
    "print(t_value[0:10])\n",
    "print(sim_p[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type II Error and the Size of the Effect \n",
    "# number of simulations where the null was rejected at 5% significance level\n",
    "n_rej = np.sum(sim_p < 0.05)\n",
    "\n",
    "prop_rej = (n_rej / float(n_sims))\n",
    "\n",
    "print(\"Power of the TEST: \", prop_rej)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here using scipy.stats.tttest_ind to generate the relation between sample size and the power of the test in a two-sided test.  Be aware not to use scipy.stats in one-sided test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "# start at 50 participants\n",
    "n_per_group = 50\n",
    "\n",
    "# effect size = 0.2\n",
    "group_means = [0.0, 0.2]\n",
    "group_sigmas = [1.0, 1.0]\n",
    "\n",
    "n_groups = len(group_means)\n",
    "\n",
    "# number of simulations\n",
    "n_sims = 100\n",
    "\n",
    "# power level that we would like to reach\n",
    "desired_power = 0.99\n",
    "\n",
    "# initialise the power for the current sample size to a small value\n",
    "current_power = 0.0\n",
    "\n",
    "# keep iterating until desired power is obtained\n",
    "while current_power < desired_power:\n",
    "\n",
    "    data = np.empty([n_sims, n_per_group, n_groups])\n",
    "    data.fill(np.nan)\n",
    "\n",
    "    for i_group in range(n_groups):\n",
    "\n",
    "        data[:, :, i_group] = np.random.normal(\n",
    "            loc=group_means[i_group],\n",
    "            scale=group_sigmas[i_group],\n",
    "            size=[n_sims, n_per_group]\n",
    "        )\n",
    "\n",
    "    result = scipy.stats.ttest_ind(\n",
    "        data[:, :, 0],\n",
    "        data[:, :, 1],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    sim_p = result[1]\n",
    "\n",
    "    # number of simulations where the null was rejected\n",
    "    n_rej = np.sum(sim_p < 0.05)\n",
    "\n",
    "    prop_rej = n_rej / float(n_sims)\n",
    "\n",
    "    current_power = prop_rej\n",
    "\n",
    "    print (\"With {n:d} samples per group, power = {p:.3f}\".format(\n",
    "        n=n_per_group,\n",
    "        p=current_power))\n",
    "\n",
    "    # increase the number of samples by one for the next iteration of the loop\n",
    "    n_per_group += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the power of the test against different sample size and effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate power curves for varying sample and effect size\n",
    "\n",
    "\n",
    "# parameters for power analysis\n",
    "effect_sizes = np.array([0.01, 0.1, 0.25, 0.5, 0.8, 1, 2])\n",
    "sample_sizes = np.array(range(5, 1000))\n",
    "\n",
    "# calculate power curves from multiple power analyses\n",
    "analysis = pyplot.figure()\n",
    "ax = analysis.add_subplot(1,1,1)\n",
    "analysis = TTestIndPower().plot_power(dep_var='nobs', \n",
    "                                      nobs=sample_sizes, \n",
    "                                      effect_size=effect_sizes,\n",
    "                                      alpha=0.025,\n",
    "                                      ax=ax, title='Power of t-Test' + '\\n' + r'$\\alpha = 0.025$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calaculate the minimum size of sample to reach certain threshold level of power of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sample size needed to meet the parameters of the experiment for the analysis \n",
    "effect_size = 0.1\n",
    "alpha = 0.05 # significance level\n",
    "power = 0.95\n",
    "\n",
    "power_analysis = TTestIndPower()\n",
    "sample_size = power_analysis.solve_power(effect_size = effect_size, \n",
    "                                         power = power, \n",
    "                                         alpha = alpha)\n",
    "\n",
    "print('Required sample size: {0:.2f}'.format(sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the power of the test against different sample size and effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power vs. number of observations for different significant level\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "fig = TTestIndPower().plot_power(dep_var='nobs',\n",
    "                                 nobs= np.arange(2, 100),\n",
    "                                 effect_size=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "                                 alpha=0.01,\n",
    "                                 ax=ax, title='Power of t-Test' + '\\n' + r'$\\alpha = 0.01$')\n",
    "ax.get_legend().remove()\n",
    "ax = fig.add_subplot(2,2,2)\n",
    "fig = TTestIndPower().plot_power(dep_var='nobs',\n",
    "                                 nobs= np.arange(2, 100),\n",
    "                                 effect_size=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "                                 alpha=0.03,\n",
    "                                 ax=ax, title=r'$\\alpha = 0.03$') \n",
    "ax.get_legend().remove()\n",
    "fig.subplots_adjust(top = 2.5, wspace = 0.5, hspace = 0.5)\n",
    "\n",
    "ax = fig.add_subplot(2,2,3)\n",
    "fig = TTestIndPower().plot_power(dep_var='nobs',\n",
    "                                 nobs= np.arange(2, 100),\n",
    "                                 effect_size=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "                                 alpha=0.04,\n",
    "                                 ax=ax, title=r'$\\alpha = 0.04$')\n",
    "ax.get_legend().remove()\n",
    "ax = fig.add_subplot(2,2,4)\n",
    "fig = TTestIndPower().plot_power(dep_var='nobs',\n",
    "                                 nobs= np.arange(2, 100),\n",
    "                                 effect_size=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]),\n",
    "                                 alpha=0.06,\n",
    "                                 ax=ax, title=r'$\\alpha = 0.06$') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
